{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "286f4be9-93ba-4f63-96f1-868be7cfa711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI, Ollama\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d8453ba-6598-42a7-b89b-41732105df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral= OllamaLLM(model=\"gemma:2b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b1c37c2-b6c7-4f9b-b145-d6c8448a9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template= \"\"\"Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input _variables=[\"question\"])llm_chain = LLMChain(prompt=prompt, llm=mistral)question =\"What is artificial intelligence?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "361df62d-6aca-4585-b9e7-a3f4783d2b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a definition of artificial intelligence (AI):\n",
      "\n",
      "**Artificial intelligence (AI)** is the simulation of human intelligence processes by computers. This involves the ability of a computer system to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
      "\n",
      "**Key characteristics of AI include:**\n",
      "\n",
      "* **Learning:** AI systems can learn from data and experience.\n",
      "* **Problem-solving:** They can identify and solve problems in a variety of domains.\n",
      "* **Reasoning:** AI systems can reason and draw conclusions based on logical reasoning.\n",
      "* **Communication:** AI systems can interact with humans in natural language.\n",
      "* **Adaptability:** AI systems can learn and adapt to new situations and tasks.\n",
      "\n",
      "**Types of AI:**\n",
      "\n",
      "* **Narrow AI:** Also known as machine learning, this type of AI focuses on specific tasks, such as image recognition or natural language processing.\n",
      "* **General AI:** This type of AI aims to create machines with human-level intelligence and reasoning capabilities.\n",
      "* **Super AI:** A hypothetical future AI that surpasses human intelligence in all aspects.\n",
      "\n",
      "**Applications of AI:**\n",
      "\n",
      "* **Automation:** AI can automate tasks such as customer service, data entry, and manufacturing.\n",
      "* **Medicine:** AI is used in medical diagnosis, drug discovery, and personalized treatment.\n",
      "* **Finance:** AI is employed in risk management, fraud detection, and portfolio optimization.\n",
      "* **Entertainment:** AI powers movie recommendation systems and virtual assistants.\n",
      "* **Transportation:** AI-powered self-driving cars and traffic management systems are becoming more common.\n",
      "\n",
      "**Ethical considerations of AI:**\n",
      "\n",
      "* **Bias:** AI systems can inherit biases from the data they are trained on, leading to unfair or discriminatory outcomes.\n",
      "* **Transparency:** AI systems can be complex and opaque, making it difficult to understand and audit their decisions.\n",
      "* **Job displacement:** AI automation may displace certain jobs, leading to economic and social challenges.\n",
      "\n",
      "**In conclusion, AI is a rapidly evolving field that involves the development of machines that exhibit human intelligence and capabilities.**\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "question =\"What is artificial intelligence?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8752ed0-74ab-479c-9b87-5a3a2565d5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter city name:  beijing\n",
      "Enter date:  04/20/2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The weather forecast for beijing on 04/20/2023 is:\n",
      "I cannot access real-time information, therefore I cannot provide a weather forecast for Beijing on 04/20/2023.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "# mistral = Mistral(model_path=\"~/mistral-models/mistral-base\")  # 直接导入本地下载好的模型\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "\n",
    "template = \"\"\"Question: What will the weather be like in {city} on {date}?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"city\", \"date\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "city = input(\"Enter city name: \")\n",
    "date = input(\"Enter date: \")\n",
    "\n",
    "print(f\"\\nThe weather forecast for {city} on {date} is:\")\n",
    "print(llm_chain.run(city=city, date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83690ba1-c1ca-4fb4-8e28-237c054b19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Chatbot! Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! I am here to assist you with any questions or tasks you may have. Is there anything I can help you with today?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWelcome to the Chatbot! Type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to quit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "template = \"\"\"User: {message}\n",
    "Assistant: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"message\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "print(\"Welcome to the Chatbot! Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    message = input(\"User: \")\n",
    "\n",
    "    if message.lower() == 'exit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # print(\"Assistant:\", llm_chain.run(message=message).strip())\n",
    "    print(\"Assistant:\", llm_chain.run(message=message).strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbab595-dad3-4330-b8a4-849125410fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cabb1d-1c60-4b3a-9f19-567aa52177a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
