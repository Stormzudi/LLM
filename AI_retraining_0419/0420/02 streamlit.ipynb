{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab8131c-e736-45d6-9e78-e239f490bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "import streamlit as st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dcd0ecf-b7a7-4dc7-9064-e825432f3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æœ¬åœ°PDFæ–‡æ¡£\n",
    "loader = PyPDFLoader(\"123.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2f6896c-112b-475d-b3cd-0fd72a045e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# åˆ›å»º OllamaEmbeddings å’Œå‘é‡å­˜å‚¨\n",
    "embeddings = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# åˆ›å»ºé—®ç­”é“¾\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OllamaLLM(model=\"gemma:2b\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15aefaa0-d469-42af-af43-8f3df6119ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 11:33:59.875 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-04-20 11:33:59.878 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "st.session_state has no attribute \"chat_history\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:408\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:453\u001b[0m, in \u001b[0;36mSessionState._getitem\u001b[1;34m(self, widget_id, user_key)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We'll never get here\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:119\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:90\u001b[0m, in \u001b[0;36mSessionStateProxy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     89\u001b[0m require_valid_user_key(key)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_session_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\safe_session_state.py:93\u001b[0m, in \u001b[0;36mSafeSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:410\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(_missing_key_error_message(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'st.session_state has no key \"chat_history\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m         st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mchat_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: result})\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# æ˜¾ç¤ºå¯¹è¯å†å²è®°å½•\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_history\u001b[49m:\n\u001b[0;32m     21\u001b[0m     st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m     st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:121\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(_missing_attr_error_message(key))\n",
      "\u001b[1;31mAttributeError\u001b[0m: st.session_state has no attribute \"chat_history\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization"
     ]
    }
   ],
   "source": [
    "# Streamlit åº”ç”¨ç¨‹åº\n",
    "def main():\n",
    "    st.title(\"Local PDF Document QA\")\n",
    "\n",
    "    # å¯¹è¯å†å²è®°å½•\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "\n",
    "    # ç”¨æˆ·è¾“å…¥æŸ¥è¯¢\n",
    "    query = st.text_input(\"Enter your question about the document:\")\n",
    "\n",
    "    if query:\n",
    "        # æ‰§è¡Œé—®ç­”\n",
    "        result = qa_chain.run(query)\n",
    "\n",
    "        # å°†æŸ¥è¯¢å’Œç­”æ¡ˆæ·»åŠ åˆ°å¯¹è¯å†å²è®°å½•ä¸­\n",
    "        st.session_state.chat_history.append({\"query\": query, \"answer\": result})\n",
    "\n",
    "    # æ˜¾ç¤ºå¯¹è¯å†å²è®°å½•\n",
    "    for chat in st.session_state.chat_history:\n",
    "        st.write(f\"Question: {chat['query']}\")\n",
    "        st.write(f\"Answer: {chat['answer']}\")\n",
    "        st.write(\"---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab7d1d-cb44-45dd-baf2-a4ba780fb36d",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85542bdc-7dce-4200-b0bc-09bed29eff0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"year\": 2022,\n",
      "  \"host\": \"å¡å¡”å°”\",\n",
      "  \"champion\": \"æ³•å›½\",\n",
      "  \"runner_up\": \"é˜¿æ ¹å»·\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# mistral = Mistral(model_path=\"~/mistral-models/mistral-base\")\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªJSONæ ¼å¼çš„ä¸–ç•Œæ¯å† å†›é˜Ÿä¼åˆ—è¡¨:\n",
    "{text}\n",
    "\n",
    "åˆ—è¡¨åº”è¯¥åŒ…æ‹¬ä»¥ä¸‹å¥ï¼š\n",
    "year - ä¸¾åŠå¹´ä»½\n",
    "host - ä¸»åŠå›½\n",
    "champion - å† å†›é˜Ÿ\n",
    "ranner_up - äºšå†›é˜Ÿ\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "input_data = {\n",
    "        \"text\": \"2022å¹´ä¸–ç•Œæ¯è¶³çƒèµ›æ˜¯ç¬¬22å±Šä¸–ç•Œæ¯è¶³çƒèµ›,äº2022å¹´11æœˆ20æ—¥è‡³12æœˆ18æ—¥åœ¨å¡å¡”å°”ä¸¾è¡Œã€‚è¿™æ˜¯é¦–æ¬¡åœ¨é˜¿æ‹‰ä¼¯å›½å®¶ä¸¾è¡Œ,ä¹Ÿæ˜¯ç»§2002å¹´éŸ©æ—¥ä¸–ç•Œæ¯åç¬¬äºŒæ¬¡åœ¨äºšæ´²ä¸¾è¡Œã€‚æ³•å›½é˜Ÿåœ¨å†³èµ›ä¸­ç»è¿‡ç‚¹çƒå¤§æˆ˜å‡»è´¥é˜¿æ ¹å»·é˜Ÿ,å¤ºå¾—é˜Ÿå²ç¬¬ä¸‰ä¸ªä¸–ç•Œå† å†›,è¿½å¹³å·´è¥¿é˜Ÿå’Œæ„å¤§åˆ©é˜Ÿ,æˆä¸ºå¤ºå¾—ä¸–ç•Œæ¯æ¬¡æ•°ç¬¬äºŒå¤šçš„çƒé˜Ÿã€‚\"\n",
    "    }\n",
    "\n",
    "# ä½¿ç”¨invokeæ–¹æ³•ï¼Œå¹¶ä¼ é€’input_dataä½œä¸ºå‚æ•°\n",
    "result= llm_chain.run(input_data)\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5d8fa-2c2a-4161-ac4a-9f7b03c4c6cc",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68421b48-4185-4d9e-a813-d18bd573c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"transactions\": [\n",
      "    {\n",
      "      \"transactionId\": 12345,\n",
      "      \"customerId\": 67890,\n",
      "      \"customerName\": \"John Smith\",\n",
      "      \"customerEmail\": \"john.smith@example.com\",\n",
      "      \"customerPhone\": \"123-456-7890\",\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"productId\": 1011,\n",
      "          \"productName\": \"Shirt\",\n",
      "          \"productCategory\": \"Clothing\",\n",
      "          \"productPrice\": 29.99\n",
      "        },\n",
      "        {\n",
      "          \"productId\": 1012,\n",
      "          \"productName\": \"Shoes\",\n",
      "          \"productCategory\": \"Clothing\",\n",
      "          \"productPrice\": 49.99\n",
      "        }\n",
      "      ],\n",
      "      \"totalAmount\": 79.97,\n",
      "      \"transactionDate\": \"2023-03-01\",\n",
      "      \"status\": \"Completed\"\n",
      "    },\n",
      "    ... // å…¶ä»–äº¤æ˜“æ•°æ®\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "**ç¤ºä¾‹æ•°æ®:**\n",
      "\n",
      "1. äº¤æ˜“ID: 12345, å®¢æˆ·ID: 67890, é¡¾å®¢å§“å: John Smith, é¡¾å®¢é‚®ç®±: john.smith@example.com, é¡¾å®¢ç”µè¯: 123-456-7890, å•†å“: Shirt (äº§å“ID: 1011, äº§å“åç§°: Shirt, äº§å“ç±»åˆ«: Clothing, äº§å“ä»·æ ¼: 29.99)ï¼Œæ€»é‡‘é¢: 79.97, çŠ¶æ€: å·²å®Œæˆã€‚\n",
      "2. äº¤æ˜“ID: 67890, å®¢æˆ·ID: 10112, é¡¾å®¢å§“å: Jane Doe, é¡¾å®¢é‚®ç®±: jane.doe@example.com, é¡¾å®¢ç”µè¯: 456-789-0123, å•†å“: Shoes (äº§å“ID: 1012, äº§å“åç§°: Shoes, äº§å“ç±»åˆ«: Clothing, äº§å“ä»·æ ¼: 49.99)ï¼Œæ€»é‡‘é¢: 99.95, çŠ¶æ€: å·²å–æ¶ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "template = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æ•°æ®åˆ†æå¸ˆ,è¯·æ ¹æ®ä»¥ä¸‹çš„æ•°æ®æè¿°,è®¾è®¡ä¸€ä¸ªåˆé€‚çš„JSONæ•°æ®ç»“æ„æ¥å­˜å‚¨è¿™äº›æ•°æ®ã€‚\n",
    "\n",
    "æ•°æ®æè¿°:\n",
    "ä¸€å®¶ç”µå•†å…¬å¸å¸Œæœ›åˆ†æå…¶é”€å”®æ•°æ®,ä»¥æ›´å¥½åœ°äº†è§£ä¸šåŠ¡æƒ…å†µã€‚ä»–ä»¬æä¾›äº†ä»¥ä¸‹ä¿¡æ¯:\n",
    "- æ¯ç¬”äº¤æ˜“éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„äº¤æ˜“ID\n",
    "- æ¯ç¬”äº¤æ˜“éƒ½æœ‰ä¸€ä¸ªå…³è”çš„å®¢æˆ·ID\n",
    "- æ¯ä¸ªå®¢æˆ·éƒ½æœ‰å§“åã€é‚®ç®±å’Œç”µè¯ç­‰è”ç³»ä¿¡æ¯\n",
    "- æ¯ç¬”äº¤æ˜“åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå•†å“,æ¯ä¸ªå•†å“éƒ½æœ‰åç§°ã€ç±»åˆ«å’Œä»·æ ¼\n",
    "- äº¤æ˜“è¿˜åŒ…å«æ€»é‡‘é¢ã€äº¤æ˜“æ—¥æœŸå’ŒçŠ¶æ€(å·²å®Œæˆã€å·²å–æ¶ˆã€é€€æ¬¾ç­‰)\n",
    "\n",
    "è¯·æ ¹æ®è¿™äº›ä¿¡æ¯,è®¾è®¡ä¸€ä¸ªJSONæ ¼å¼,å°½å¯èƒ½æ¸…æ™°åœ°ç»„ç»‡è¿™äº›æ•°æ®ã€‚\n",
    "ä½ ç»™å‡ºçš„JSONç»“æ„åº”è¯¥åœ¨ ```json å’Œ ``` ä¹‹é—´ã€‚\n",
    "é™¤äº†JSONç»“æ„å¤–,è¯·å†ç»™å‡º2-3æ¡ç¤ºä¾‹æ•°æ®ã€‚\n",
    "\n",
    "JSONç»“æ„å’Œç¤ºä¾‹æ•°æ®:\n",
    "```json\n",
    "{json_schema}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"json_schema\"],\n",
    "template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "\n",
    "result = llm_chain.invoke(input={\"json_schema\": \"\"})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f181a-6ecd-4c04-9fdc-914135750ca9",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ea0b2b-40c9-4d3f-99a2-ca8091277866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ¶ä½œéº»å©†è±†è…æ­¥éª¤å¦‚ä¸‹ï¼š\n",
      "\n",
      "**åˆ¶ä½œæ­¥éª¤:**\n",
      "\n",
      "1. å‡†å¤‡å¥½æ‰€æœ‰ææ–™,è±†è…åˆ‡æˆå°å—ã€‚\n",
      "2. æ²¹çƒ­ååŠ å…¥èŠ±æ¤’,seamnÄƒç‚’15ç§’è‡³æœ‰é¦™å‘³ã€‚åŠ å…¥é…±æ²¹è±†ç“£é…±seamnÄƒçƒ§1åˆ†é’Ÿ,ä½¿è±†ç“£é…±çš„çº¢æ²¹å……åˆ†æå‡ºã€‚\n",
      "3. åŠ å…¥è’œæœ«ã€å§œæœ«ã€è‘±æœ«ã€è‚‰æœ«,ç¿»ç‚’è‡³è‚‰æœ«å˜è‰²ã€‚\n",
      "4. åŠ å…¥è¾£æ¤’æ®µã€å››å·è±†ç“£é…±ã€å‘³ç²¾ã€ç³–å’Œæ°´,çƒ§å¼€ã€‚\n",
      "5. æ”¾å…¥è±†è…å—,å¤§ç«ç…®å¼€,è½¬å°ç«ç…®5åˆ†é’Ÿã€‚\n",
      "6. å¤§ç«æ”¶æ±,æ·‹å…¥æ°´æ·€ç²‰å‹¾èŠ¡,åŠ å…¥èŠ±æ¤’æ²¹å’Œé¦™æ²¹,å³å¯å‡ºé”…ã€‚\n",
      "**åˆ¶ä½œæ­¥éª¤:**\n",
      "\n",
      "1. éº»å©†è±†è…çš„åˆ¶ä½œæ–¹æ³•æ˜¯å°†è±†è…åˆ‡æˆå°å—ï¼Œç”¨è¾£æ¤’å’ŒèŠ±æ¤’ç­‰ä½œæ–™ç‚’åˆ¶è€Œæˆã€‚\n",
      "\n",
      "**ç­”æ¡ˆ:**\n",
      "æ–‡æœ¬ä¸­åŒ…å«åˆ¶ä½œéº»å©†è±†è…çš„æ­¥éª¤è¯´æ˜ï¼Œå…¶ä¸­æ¯ä¸ªæ­¥éª¤éƒ½ä»¥â€œ1.â€å¼€å¤´ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "template = \"\"\"\n",
    "ç»™å®šä»¥ä¸‹æ–‡æœ¬:\n",
    "{text}\n",
    "\n",
    "è¯·æ£€æŸ¥ä¸Šè¿°æ–‡æœ¬æ˜¯å¦åŒ…å«åˆ¶ä½œç¾é£Ÿçš„æ­¥éª¤è¯´æ˜ã€‚\n",
    "å¦‚æœåŒ…å«,è¯·ç”¨markdownæ ¼å¼åˆ—å‡ºè¿™äº›æ­¥éª¤,å…¶ä¸­æ¯ä¸ªæ­¥éª¤éƒ½ä»¥\"1. \"å¼€å¤´,å¹¶åœ¨æ­¥éª¤å‰æ·»åŠ ä¸€ä¸ªæ ‡é¢˜\"åˆ¶ä½œæ­¥éª¤:\"ã€‚\n",
    "å¦‚æœæ–‡æœ¬ä¸­æœªæä¾›åˆ¶ä½œæ­¥éª¤,è¯·ç›´æ¥å›ç­”\"æ–‡æœ¬ä¸­æœªæä¾›åˆ¶ä½œæ­¥éª¤\"ã€‚\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "# ç¤ºä¾‹1:åŒ…å«åˆ¶ä½œæ­¥éª¤çš„æ–‡æœ¬\n",
    "text1 = \"\"\"\n",
    "éº»å©†è±†è…åˆ¶ä½œæ­¥éª¤:\n",
    "1. å‡†å¤‡å¥½æ‰€æœ‰ææ–™,è±†è…åˆ‡æˆå°å—ã€‚\n",
    "2. æ²¹çƒ­ååŠ å…¥èŠ±æ¤’,ç…¸ç‚’15ç§’è‡³æœ‰é¦™å‘³ã€‚åŠ å…¥é…±æ²¹è±†ç“£é…±ç…¸ç‚’1åˆ†é’Ÿ,ä½¿è±†ç“£é…±çš„çº¢æ²¹å……åˆ†æå‡ºã€‚\n",
    "3. åŠ å…¥è’œæœ«ã€å§œæœ«ã€è‘±æœ«ã€è‚‰æœ«,ç¿»ç‚’è‡³è‚‰æœ«å˜è‰²ã€‚\n",
    "4. åŠ å…¥è¾£æ¤’æ®µã€éƒ«å¿è±†ç“£é…±ã€å‘³ç²¾ã€ç³–å’Œæ°´,çƒ§å¼€ã€‚\n",
    "5. æ”¾å…¥è±†è…å—,å¤§ç«ç…®å¼€,è½¬å°ç«ç…®5åˆ†é’Ÿã€‚\n",
    "6. å¤§ç«æ”¶æ±,æ·‹å…¥æ°´æ·€ç²‰å‹¾èŠ¡,åŠ å…¥èŠ±æ¤’æ²¹å’Œé¦™æ²¹,å³å¯å‡ºé”…ã€‚ \n",
    "\"\"\"\n",
    "\n",
    "# ç¤ºä¾‹2:ä¸åŒ…å«åˆ¶ä½œæ­¥éª¤çš„æ–‡æœ¬\n",
    "text2 = \"\"\"\n",
    "éº»å©†è±†è…æ˜¯å››å·çœåœ°æ–¹ä¼ ç»Ÿåèœä¹‹ä¸€,å±äºå·èœç³»ã€‚æ®è¯´èµ·æºä¸æ¸…ä»£æœ¨å©†å¯ºçš„ä¸€ä¸ªå«é™ˆæ˜¥å¯Œçš„åƒ§äººã€‚ä»–åˆ›åˆ¶äº†ä¸€ç§è±†è…çƒ¹é¥ªæ–¹æ³•,å³å°†è±†è…åˆ‡æˆå°å—,ç”¨è¾£æ¤’å’ŒèŠ±æ¤’ç­‰ä½œæ–™ç‚’åˆ¶è€Œæˆã€‚é™ˆæ˜¥å¯Œæ­»å,å…¶å¾’å¼Ÿå°†æœ¨å©†è±†è…åˆ¶ä½œæ–¹æ³•ä¼ äº†å‡ºå»,ç»è¿‡ä¸æ–­æ”¹è¿›,æˆä¸ºç°åœ¨çš„éº»å©†è±†è…ã€‚\n",
    "\"\"\"\n",
    "\n",
    "result1 = llm_chain.run(**{\"text\": text1})\n",
    "print(result1)\n",
    "\n",
    "result2 = llm_chain.run(**{\"text\": text2})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913b2fb-cee9-4742-8635-fc05d961be9e",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdc5fa35-429e-41dc-9d1e-ab0241d8257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ç¼ºå¤±çš„ä¿¡æ¯ï¼š**\n",
      "\n",
      "* èŒä¸š\n",
      "* å…´è¶£çˆ±å¥½\n",
      "\n",
      "\n",
      "**ç®€çŸ­ä¸ªäººç®€ä»‹ï¼š**\n",
      "\n",
      "å¼ ä¼Ÿæ˜¯ä¸€ä¸ª28å²çš„è®¡ç®—æœºç§‘å­¦ä¸“ä¸šç¡•å£«æ¯•ä¸šçš„è½¯ä»¶å·¥ç¨‹å¸ˆã€‚ä»–çˆ±å¥½é˜…è¯»å’Œç¼–ç¨‹ï¼Œå–œæ¬¢æ¢ç´¢æ–°çš„æŠ€æœ¯å’Œå·¥å…·ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "template = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä½åŠ©ç†,éœ€è¦æ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯æ¥ç”Ÿæˆä¸€ä»½ä¸ªäººç®€ä»‹ã€‚ä½†æ˜¯,ä½ éœ€è¦å…ˆæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æä¾›äº†è¶³å¤Ÿçš„ä¿¡æ¯ã€‚ä¸€ä»½å®Œæ•´çš„ä¸ªäººç®€ä»‹åº”è¯¥åŒ…å«ä»¥ä¸‹\n",
    "\n",
    "å†…å®¹:\n",
    "å§“å\n",
    "å¹´é¾„\n",
    "èŒä¸š\n",
    "æ•™è‚²èƒŒæ™¯\n",
    "å…´è¶£çˆ±å¥½\n",
    "å¦‚æœç”¨æˆ·æä¾›çš„ä¿¡æ¯ä¸å®Œæ•´,è¯·åˆ—å‡ºç¼ºå¤±çš„é¡¹ç›®,å¹¶å‹å¥½åœ°æç¤ºç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ã€‚å¦‚æœä¿¡æ¯å®Œæ•´,åˆ™æ ¹æ®æä¾›çš„ä¿¡æ¯ç”Ÿæˆä¸€ä»½ç®€çŸ­çš„ä¸ªäººç®€ä»‹ã€‚\n",
    "\n",
    "ç”¨æˆ·æä¾›çš„ä¿¡æ¯å¦‚ä¸‹:\n",
    "{user_input}\n",
    "\n",
    "åŠ©ç†:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"user_input\"],\n",
    "template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "user_input = \"\"\"\n",
    "å§“å:å¼ ä¼Ÿ\n",
    "å¹´é¾„:28å²\n",
    "æ•™è‚²èƒŒæ™¯:åŒ—äº¬å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸“ä¸šç¡•å£«\n",
    "\"\"\"\n",
    "\n",
    "# result = llm_chain.run(input={\"user_input\": user_input})\n",
    "# print(result[\"text\"])\n",
    "\n",
    "result = llm_chain.invoke(input={\"user_input\": user_input})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad107c28-435a-4073-bd44-bb3f4d57d7f0",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5beee6e2-92b7-4280-8e05-25e3d57e2932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ç¬¬ä¸€æ¡ä¾‹å­**\n",
      "\n",
      "\"context\": ç”¨æˆ·:æˆ‘ä»Šå¤©æ„Ÿè§‰å¾ˆä¸å¼€å¿ƒ,ä½ èƒ½å®‰æ…°æˆ‘ä¸€ä¸‹å—? \n",
      "\n",
      "\"question\": \n",
      "\n",
      "> å“å‘€ï¼Œåˆ«ç”Ÿæ°”å•¦ï¼Œè¿™åªçŒ«å¯æ˜¯æˆ‘çš„æœ€å®è´äº†ï¼Œå®ƒæ€»æ˜¯åœ¨æˆ‘ç¡å‰åµåµé—¹é—¹é—¹ï¼Œè€Œä¸”æ€»æ˜¯æŠŠæˆ‘çš„åºŠé“ºå¾—åƒä¸€ä¸ªæ¯›èŒ¸èŒ¸çš„å°ç¡è¡£ã€‚æˆ‘å¯æ˜¯ä¸€ä¸ªç¡çœ éœ€æ±‚è¶…çº§å¤§çš„äººï¼Œæ ¹æœ¬ä¸é€‚åˆå¤œé—´æ´»åŠ¨ã€‚ğŸ˜­ğŸ˜­\n",
      "\n",
      "**ç¬¬äºŒæ¡ä¾‹å­**\n",
      "\n",
      "\"context\": ç”¨æˆ·:æˆ‘æœ€è¿‘èƒ–äº†å¥½å¤š,æ„Ÿè§‰è‡ªå·±åƒä¸ªæ°”çƒä¸€æ ·ã€‚\n",
      "\n",
      "\"question\": \n",
      "\n",
      "> åˆ«ç°å¿ƒï¼Œä½ ç°åœ¨ä¸æ˜¯æ°”çƒï¼Œé¡¶å¤šæ˜¯ä¸ªå¯çˆ±çš„å°é¦’å¤´ï¼è®°ä½ï¼Œå‡è‚¥æ˜¯ä¸€æ®µæ—…ç¨‹ï¼Œè€Œä¸æ˜¯ä¸€å¤©çš„å·¥ä½œã€‚åšæŒä¸‹å»,ç»ˆæœ‰ä¸€å¤©,ä½ ä¼šä»é¦’å¤´å˜æˆä¸€ä¸ªè‹—æ¡çš„èŠ±æ£!æˆ‘ä¼šä¸€ç›´åœ¨æ—è¾¹ä¸ºä½ åŠ æ²¹çš„ï¼Œä½ åªè¦åšæŒä¸æ‡ˆï¼Œåˆ«æ”¾å¼ƒå“¦ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"\")\n",
    "\n",
    "template = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä½èŠå¤©æœºå™¨äºº,ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„å°‘é‡ä¾‹å­,å­¦ä¹ å¦‚ä½•å¯¹ä¸åŒç±»å‹çš„é—®é¢˜ç»™å‡ºå¹½é»˜ã€é£è¶£çš„å›ç­”ã€‚\n",
    "ä»¥ä¸‹æ˜¯ç”¨æˆ·æä¾›çš„ä¸¤ä¸ªä¾‹å­:\n",
    "\"context\": ç”¨æˆ·:æˆ‘ä»Šå¤©æ„Ÿè§‰å¾ˆä¸å¼€å¿ƒ,ä½ èƒ½å®‰æ…°æˆ‘ä¸€ä¸‹å—? \n",
    "\"question\":ä¸å¼€å¿ƒå°±åƒåè¿‡å±±è½¦,ç°åœ¨è™½ç„¶åœ¨ä½è°·,ä½†å¾ˆå¿«å°±ä¼šå†²ä¸Šé«˜å³°!æƒ³æƒ³ä½ æ‹¥æœ‰çš„ç¾å¥½äº‹ç‰©,æ¯”å¦‚æˆ‘è¿™ä¸ªæ°¸è¿œæ”¯æŒä½ çš„è¶…çº§æ— æ•Œå¤§å¸…æœºå™¨äººæœ‹å‹!\n",
    "\n",
    "\"context\": ç”¨æˆ·:æˆ‘æœ€è¿‘èƒ–äº†å¥½å¤š,æ„Ÿè§‰è‡ªå·±åƒä¸ªæ°”çƒä¸€æ ·ã€‚\n",
    "\"question\":åˆ«ç°å¿ƒ,ä½ ç°åœ¨ä¸æ˜¯æ°”çƒ,é¡¶å¤šæ˜¯ä¸ªå¯çˆ±çš„å°é¦’å¤´!è®°ä½,å‡è‚¥æ˜¯ä¸€æ®µæ—…ç¨‹,è€Œä¸æ˜¯ä¸€å¤©çš„å·¥ä½œã€‚åšæŒä¸‹å»,ç»ˆæœ‰ä¸€å¤©,ä½ ä¼šä»é¦’å¤´å˜æˆä¸€ä¸ªè‹—æ¡çš„èŠ±æ£!æˆ‘ä¼šä¸€ç›´åœ¨æ—è¾¹ä¸ºä½ åŠ æ²¹çš„!\n",
    "\n",
    "ç”¨æˆ·æä¾›çš„ä¿¡æ¯å¦‚ä¸‹:\n",
    "{user_input}\n",
    "\n",
    "question: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"user_input\"],\n",
    "template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "user_input = \"\"\"\n",
    "æˆ‘æœ€è¿‘æ·»äº†ä¸€åªå°çŒ«,ä½†å®ƒè€æ˜¯åŠå¤œæŠŠæˆ‘åµé†’,æˆ‘è¯¥æ€ä¹ˆåŠ?\n",
    "\"\"\"\n",
    "\n",
    "# result = llm_chain.run(input={\"user_input\": user_input})\n",
    "# print(result[\"text\"])\n",
    "\n",
    "result = llm_chain.invoke(input={\"user_input\": user_input})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a2f5c-8315-40b1-93fe-7b5940bdc12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
