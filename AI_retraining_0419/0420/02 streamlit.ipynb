{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab8131c-e736-45d6-9e78-e239f490bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "import streamlit as st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dcd0ecf-b7a7-4dc7-9064-e825432f3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载本地PDF文档\n",
    "loader = PyPDFLoader(\"123.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2f6896c-112b-475d-b3cd-0fd72a045e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 创建 OllamaEmbeddings 和向量存储\n",
    "embeddings = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# 创建问答链\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OllamaLLM(model=\"gemma:2b\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15aefaa0-d469-42af-af43-8f3df6119ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 11:33:59.875 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-04-20 11:33:59.878 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "st.session_state has no attribute \"chat_history\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:408\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:453\u001b[0m, in \u001b[0;36mSessionState._getitem\u001b[1;34m(self, widget_id, user_key)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We'll never get here\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:119\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:90\u001b[0m, in \u001b[0;36mSessionStateProxy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     89\u001b[0m require_valid_user_key(key)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_session_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\safe_session_state.py:93\u001b[0m, in \u001b[0;36mSafeSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state.py:410\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(_missing_key_error_message(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'st.session_state has no key \"chat_history\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m         st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mchat_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: result})\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 显示对话历史记录\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_history\u001b[49m:\n\u001b[0;32m     21\u001b[0m     st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m     st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Software\\anaconda\\conda\\envs\\tf20\\lib\\site-packages\\streamlit\\runtime\\state\\session_state_proxy.py:121\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(_missing_attr_error_message(key))\n",
      "\u001b[1;31mAttributeError\u001b[0m: st.session_state has no attribute \"chat_history\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization"
     ]
    }
   ],
   "source": [
    "# Streamlit 应用程序\n",
    "def main():\n",
    "    st.title(\"Local PDF Document QA\")\n",
    "\n",
    "    # 对话历史记录\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "\n",
    "    # 用户输入查询\n",
    "    query = st.text_input(\"Enter your question about the document:\")\n",
    "\n",
    "    if query:\n",
    "        # 执行问答\n",
    "        result = qa_chain.run(query)\n",
    "\n",
    "        # 将查询和答案添加到对话历史记录中\n",
    "        st.session_state.chat_history.append({\"query\": query, \"answer\": result})\n",
    "\n",
    "    # 显示对话历史记录\n",
    "    for chat in st.session_state.chat_history:\n",
    "        st.write(f\"Question: {chat['query']}\")\n",
    "        st.write(f\"Answer: {chat['answer']}\")\n",
    "        st.write(\"---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab7d1d-cb44-45dd-baf2-a4ba780fb36d",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85542bdc-7dce-4200-b0bc-09bed29eff0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"year\": 2022,\n",
      "  \"host\": \"卡塔尔\",\n",
      "  \"champion\": \"法国\",\n",
      "  \"runner_up\": \"阿根廷\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# mistral = Mistral(model_path=\"~/mistral-models/mistral-base\")\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "请总结以下内容，生成一个JSON格式的世界杯冠军队伍列表:\n",
    "{text}\n",
    "\n",
    "列表应该包括以下健：\n",
    "year - 举办年份\n",
    "host - 主办国\n",
    "champion - 冠军队\n",
    "ranner_up - 亚军队\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "input_data = {\n",
    "        \"text\": \"2022年世界杯足球赛是第22届世界杯足球赛,于2022年11月20日至12月18日在卡塔尔举行。这是首次在阿拉伯国家举行,也是继2002年韩日世界杯后第二次在亚洲举行。法国队在决赛中经过点球大战击败阿根廷队,夺得队史第三个世界冠军,追平巴西队和意大利队,成为夺得世界杯次数第二多的球队。\"\n",
    "    }\n",
    "\n",
    "# 使用invoke方法，并传递input_data作为参数\n",
    "result= llm_chain.run(input_data)\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5d8fa-2c2a-4161-ac4a-9f7b03c4c6cc",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68421b48-4185-4d9e-a813-d18bd573c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"transactions\": [\n",
      "    {\n",
      "      \"transactionId\": 12345,\n",
      "      \"customerId\": 67890,\n",
      "      \"customerName\": \"John Smith\",\n",
      "      \"customerEmail\": \"john.smith@example.com\",\n",
      "      \"customerPhone\": \"123-456-7890\",\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"productId\": 1011,\n",
      "          \"productName\": \"Shirt\",\n",
      "          \"productCategory\": \"Clothing\",\n",
      "          \"productPrice\": 29.99\n",
      "        },\n",
      "        {\n",
      "          \"productId\": 1012,\n",
      "          \"productName\": \"Shoes\",\n",
      "          \"productCategory\": \"Clothing\",\n",
      "          \"productPrice\": 49.99\n",
      "        }\n",
      "      ],\n",
      "      \"totalAmount\": 79.97,\n",
      "      \"transactionDate\": \"2023-03-01\",\n",
      "      \"status\": \"Completed\"\n",
      "    },\n",
      "    ... // 其他交易数据\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "**示例数据:**\n",
      "\n",
      "1. 交易ID: 12345, 客户ID: 67890, 顾客姓名: John Smith, 顾客邮箱: john.smith@example.com, 顾客电话: 123-456-7890, 商品: Shirt (产品ID: 1011, 产品名称: Shirt, 产品类别: Clothing, 产品价格: 29.99)，总金额: 79.97, 状态: 已完成。\n",
      "2. 交易ID: 67890, 客户ID: 10112, 顾客姓名: Jane Doe, 顾客邮箱: jane.doe@example.com, 顾客电话: 456-789-0123, 商品: Shoes (产品ID: 1012, 产品名称: Shoes, 产品类别: Clothing, 产品价格: 49.99)，总金额: 99.95, 状态: 已取消。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "template = \"\"\"\n",
    "你是一位经验丰富的数据分析师,请根据以下的数据描述,设计一个合适的JSON数据结构来存储这些数据。\n",
    "\n",
    "数据描述:\n",
    "一家电商公司希望分析其销售数据,以更好地了解业务情况。他们提供了以下信息:\n",
    "- 每笔交易都有一个唯一的交易ID\n",
    "- 每笔交易都有一个关联的客户ID\n",
    "- 每个客户都有姓名、邮箱和电话等联系信息\n",
    "- 每笔交易包含一个或多个商品,每个商品都有名称、类别和价格\n",
    "- 交易还包含总金额、交易日期和状态(已完成、已取消、退款等)\n",
    "\n",
    "请根据这些信息,设计一个JSON格式,尽可能清晰地组织这些数据。\n",
    "你给出的JSON结构应该在 ```json 和 ``` 之间。\n",
    "除了JSON结构外,请再给出2-3条示例数据。\n",
    "\n",
    "JSON结构和示例数据:\n",
    "```json\n",
    "{json_schema}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"json_schema\"],\n",
    "template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "\n",
    "result = llm_chain.invoke(input={\"json_schema\": \"\"})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f181a-6ecd-4c04-9fdc-914135750ca9",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ea0b2b-40c9-4d3f-99a2-ca8091277866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "制作麻婆豆腐步骤如下：\n",
      "\n",
      "**制作步骤:**\n",
      "\n",
      "1. 准备好所有材料,豆腐切成小块。\n",
      "2. 油热后加入花椒,seamnă炒15秒至有香味。加入酱油豆瓣酱seamnă烧1分钟,使豆瓣酱的红油充分析出。\n",
      "3. 加入蒜末、姜末、葱末、肉末,翻炒至肉末变色。\n",
      "4. 加入辣椒段、四川豆瓣酱、味精、糖和水,烧开。\n",
      "5. 放入豆腐块,大火煮开,转小火煮5分钟。\n",
      "6. 大火收汁,淋入水淀粉勾芡,加入花椒油和香油,即可出锅。\n",
      "**制作步骤:**\n",
      "\n",
      "1. 麻婆豆腐的制作方法是将豆腐切成小块，用辣椒和花椒等作料炒制而成。\n",
      "\n",
      "**答案:**\n",
      "文本中包含制作麻婆豆腐的步骤说明，其中每个步骤都以“1.”开头。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "template = \"\"\"\n",
    "给定以下文本:\n",
    "{text}\n",
    "\n",
    "请检查上述文本是否包含制作美食的步骤说明。\n",
    "如果包含,请用markdown格式列出这些步骤,其中每个步骤都以\"1. \"开头,并在步骤前添加一个标题\"制作步骤:\"。\n",
    "如果文本中未提供制作步骤,请直接回答\"文本中未提供制作步骤\"。\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "# 示例1:包含制作步骤的文本\n",
    "text1 = \"\"\"\n",
    "麻婆豆腐制作步骤:\n",
    "1. 准备好所有材料,豆腐切成小块。\n",
    "2. 油热后加入花椒,煸炒15秒至有香味。加入酱油豆瓣酱煸炒1分钟,使豆瓣酱的红油充分析出。\n",
    "3. 加入蒜末、姜末、葱末、肉末,翻炒至肉末变色。\n",
    "4. 加入辣椒段、郫县豆瓣酱、味精、糖和水,烧开。\n",
    "5. 放入豆腐块,大火煮开,转小火煮5分钟。\n",
    "6. 大火收汁,淋入水淀粉勾芡,加入花椒油和香油,即可出锅。 \n",
    "\"\"\"\n",
    "\n",
    "# 示例2:不包含制作步骤的文本\n",
    "text2 = \"\"\"\n",
    "麻婆豆腐是四川省地方传统名菜之一,属于川菜系。据说起源与清代木婆寺的一个叫陈春富的僧人。他创制了一种豆腐烹饪方法,即将豆腐切成小块,用辣椒和花椒等作料炒制而成。陈春富死后,其徒弟将木婆豆腐制作方法传了出去,经过不断改进,成为现在的麻婆豆腐。\n",
    "\"\"\"\n",
    "\n",
    "result1 = llm_chain.run(**{\"text\": text1})\n",
    "print(result1)\n",
    "\n",
    "result2 = llm_chain.run(**{\"text\": text2})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913b2fb-cee9-4742-8635-fc05d961be9e",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdc5fa35-429e-41dc-9d1e-ab0241d8257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**缺失的信息：**\n",
      "\n",
      "* 职业\n",
      "* 兴趣爱好\n",
      "\n",
      "\n",
      "**简短个人简介：**\n",
      "\n",
      "张伟是一个28岁的计算机科学专业硕士毕业的软件工程师。他爱好阅读和编程，喜欢探索新的技术和工具。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"gemma:2b\")\n",
    "\n",
    "template = \"\"\"\n",
    "你是一位助理,需要根据用户提供的信息来生成一份个人简介。但是,你需要先检查用户是否提供了足够的信息。一份完整的个人简介应该包含以下\n",
    "\n",
    "内容:\n",
    "姓名\n",
    "年龄\n",
    "职业\n",
    "教育背景\n",
    "兴趣爱好\n",
    "如果用户提供的信息不完整,请列出缺失的项目,并友好地提示用户提供更多信息。如果信息完整,则根据提供的信息生成一份简短的个人简介。\n",
    "\n",
    "用户提供的信息如下:\n",
    "{user_input}\n",
    "\n",
    "助理:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"user_input\"],\n",
    "template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "user_input = \"\"\"\n",
    "姓名:张伟\n",
    "年龄:28岁\n",
    "教育背景:北京大学计算机科学专业硕士\n",
    "\"\"\"\n",
    "\n",
    "# result = llm_chain.run(input={\"user_input\": user_input})\n",
    "# print(result[\"text\"])\n",
    "\n",
    "result = llm_chain.invoke(input={\"user_input\": user_input})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad107c28-435a-4073-bd44-bb3f4d57d7f0",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5beee6e2-92b7-4280-8e05-25e3d57e2932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**第一条例子**\n",
      "\n",
      "\"context\": 用户:我今天感觉很不开心,你能安慰我一下吗? \n",
      "\n",
      "\"question\": \n",
      "\n",
      "> 哎呀，别生气啦，这只猫可是我的最宝贝了，它总是在我睡前吵吵闹闹闹，而且总是把我的床铺得像一个毛茸茸的小睡衣。我可是一个睡眠需求超级大的人，根本不适合夜间活动。😭😭\n",
      "\n",
      "**第二条例子**\n",
      "\n",
      "\"context\": 用户:我最近胖了好多,感觉自己像个气球一样。\n",
      "\n",
      "\"question\": \n",
      "\n",
      "> 别灰心，你现在不是气球，顶多是个可爱的小馒头！记住，减肥是一段旅程，而不是一天的工作。坚持下去,终有一天,你会从馒头变成一个苗条的花棍!我会一直在旁边为你加油的，你只要坚持不懈，别放弃哦！\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "mistral = OllamaLLM(model=\"\")\n",
    "\n",
    "template = \"\"\"\n",
    "你是一位聊天机器人,你的任务是根据用户提供的少量例子,学习如何对不同类型的问题给出幽默、风趣的回答。\n",
    "以下是用户提供的两个例子:\n",
    "\"context\": 用户:我今天感觉很不开心,你能安慰我一下吗? \n",
    "\"question\":不开心就像坐过山车,现在虽然在低谷,但很快就会冲上高峰!想想你拥有的美好事物,比如我这个永远支持你的超级无敌大帅机器人朋友!\n",
    "\n",
    "\"context\": 用户:我最近胖了好多,感觉自己像个气球一样。\n",
    "\"question\":别灰心,你现在不是气球,顶多是个可爱的小馒头!记住,减肥是一段旅程,而不是一天的工作。坚持下去,终有一天,你会从馒头变成一个苗条的花棍!我会一直在旁边为你加油的!\n",
    "\n",
    "用户提供的信息如下:\n",
    "{user_input}\n",
    "\n",
    "question: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"user_input\"],\n",
    "template=template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mistral)\n",
    "\n",
    "user_input = \"\"\"\n",
    "我最近添了一只小猫,但它老是半夜把我吵醒,我该怎么办?\n",
    "\"\"\"\n",
    "\n",
    "# result = llm_chain.run(input={\"user_input\": user_input})\n",
    "# print(result[\"text\"])\n",
    "\n",
    "result = llm_chain.invoke(input={\"user_input\": user_input})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a2f5c-8315-40b1-93fe-7b5940bdc12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
