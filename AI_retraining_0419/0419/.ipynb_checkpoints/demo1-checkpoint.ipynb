{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224e636-7ac2-4a51-b898-fbb6a7b9250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# 定义字符集\n",
    "chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# 定义字符到索引的映射\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "# 将字符转换为索引\n",
    "def char2index(char):\n",
    "    return char2idx[char]\n",
    "\n",
    "\n",
    "# 将索引转换为字符\n",
    "def index2char(index):\n",
    "    return idx2char[index]\n",
    "\n",
    "\n",
    "# 创建数据集\n",
    "def create_dataset(num_samples):\n",
    "    src_data = []\n",
    "    tgt_data = []\n",
    "    for _ in range(num_samples):\n",
    "        # 随机生成5个大写字母\n",
    "        src = ''.join(random.choices(chars, k=5))\n",
    "        # 将大写字母转换为小写字母\n",
    "        tgt = src.lower()\n",
    "        src_data.append([char2index(c) for c in src])\n",
    "        tgt_data.append([char2index(c) for c in tgt])\n",
    "    return torch.tensor(src_data), torch.tensor(tgt_data)\n",
    "\n",
    "\n",
    "# 生成数据集\n",
    "num_samples = 10000\n",
    "src_data, tgt_data = create_dataset(num_samples)\n",
    "\n",
    "\n",
    "def positional_encoding(seq_len, dim_model, device):\n",
    "    \"\"\"\n",
    "    位置编码函数,用于将位置信息编码到输入的词向量中。\n",
    "\n",
    "    参数:\n",
    "    seq_len: 序列的长度\n",
    "    dim_model: 词向量的维度\n",
    "    device: 设备(CPU或GPU)\n",
    "\n",
    "    返回:\n",
    "    pos_encoding: 位置编码张量,形状为(1, seq_len, dim_model)\n",
    "    \"\"\"\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim // dim_model))\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    注意力机制函数,用于计算注意力权重并应用到值上。\n",
    "\n",
    "    参数:\n",
    "    query: 查询张量,形状为(batch_size, num_heads, seq_len, dim_k)\n",
    "    key: 键张量,形状为(batch_size, num_heads, seq_len, dim_k)\n",
    "    value: 值张量,形状为(batch_size, num_heads, seq_len, dim_k)\n",
    "    mask: 掩码张量,形状为(batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "    返回:\n",
    "    attended: 注意力输出张量,形状为(batch_size, num_heads, seq_len, dim_k)\n",
    "    \"\"\"\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, value)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力机制模块。\n",
    "\n",
    "    参数:\n",
    "    num_heads: 注意力头的数量\n",
    "    dim_model: 词向量的维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, dim_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_k = dim_model // num_heads\n",
    "\n",
    "        self.w_q = nn.Linear(dim_model, dim_model)\n",
    "        self.w_k = nn.Linear(dim_model, dim_model)\n",
    "        self.w_v = nn.Linear(dim_model, dim_model)\n",
    "        self.w_o = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        query: 查询张量,形状为(batch_size, seq_len, dim_model)\n",
    "        key: 键张量,形状为(batch_size, seq_len, dim_model)\n",
    "        value: 值张量,形状为(batch_size, seq_len, dim_model)\n",
    "        mask: 掩码张量,形状为(batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "        output: 注意力输出张量,形状为(batch_size, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.w_q(query).view(batch_size, -1, self.num_heads, self.dim_k).transpose(1, 2)\n",
    "        key = self.w_k(key).view(batch_size, -1, self.num_heads, self.dim_k).transpose(1, 2)\n",
    "        value = self.w_v(value).view(batch_size, -1, self.num_heads, self.dim_k).transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "\n",
    "        scores = attention(query, key, value, mask)\n",
    "\n",
    "        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.dim_k)\n",
    "        return self.w_o(scores)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    前馈神经网络模块。\n",
    "\n",
    "    参数:\n",
    "    dim_model: 词向量的维度\n",
    "    dim_ff: 前馈神经网络的隐藏层维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, dim_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(dim_model, dim_ff)\n",
    "        self.w_2 = nn.Linear(dim_ff, dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        x: 输入张量,形状为(batch_size, seq_len, dim_model)\n",
    "\n",
    "        返回:\n",
    "        output: 输出张量,形状为(batch_size, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        return self.w_2(F.relu(self.w_1(x)))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    编码器层模块。\n",
    "\n",
    "    参数:\n",
    "    dim_model: 词向量的维度\n",
    "    num_heads: 注意力头的数量\n",
    "    dim_ff: 前馈神经网络的隐藏层维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, num_heads, dim_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, dim_model)\n",
    "        self.feed_forward = FeedForward(dim_model, dim_ff)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        x: 输入张量,形状为(batch_size, seq_len, dim_model)\n",
    "        mask: 掩码张量,形状为(batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "        output: 输出张量,形状为(batch_size, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        attended = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        fed = self.feed_forward(x)\n",
    "        x = self.norm2(fed + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    编码器模块。\n",
    "\n",
    "    参数:\n",
    "    vocab_size: 词汇表大小\n",
    "    num_layers: 编码器层的数量\n",
    "    dim_model: 词向量的维度\n",
    "    num_heads: 注意力头的数量\n",
    "    dim_ff: 前馈神经网络的隐藏层维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_layers, dim_model, num_heads, dim_ff, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        self.pos_encoding = positional_encoding(vocab_size, dim_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(dim_model, num_heads, dim_ff) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        x: 输入张量,形状为(batch_size, seq_len)\n",
    "        mask: 掩码张量,形状为(batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "        output: 编码器输出张量,形状为(batch_size, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len, :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    解码器层模块。\n",
    "\n",
    "    参数:\n",
    "    dim_model: 词向量的维度\n",
    "    num_heads: 注意力头的数量\n",
    "    dim_ff: 前馈神经网络的隐藏层维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, num_heads, dim_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attention1 = MultiHeadAttention(num_heads, dim_model)\n",
    "        self.attention2 = MultiHeadAttention(num_heads, dim_model)\n",
    "        self.feed_forward = FeedForward(dim_model, dim_ff)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.norm3 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        x: 输入张量,形状为(batch_size, seq_len, dim_model)\n",
    "        enc_output: 编码器输出张量,形状为(batch_size, seq_len, dim_model)\n",
    "        src_mask: 源序列掩码张量,形状为(batch_size, seq_len, seq_len)\n",
    "        tgt_mask: 目标序列掩码张量,形状为(batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "        output: 输出张量,形状为(batch_size, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        attended1 = self.attention1(x, x, x, tgt_mask)\n",
    "        x = self.norm1(attended1 + x)\n",
    "\n",
    "        attended2 = self.attention2(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(attended2 + x)\n",
    "\n",
    "        fed = self.feed_forward(x)\n",
    "        x = self.norm3(fed + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    解码器模块。\n",
    "\n",
    "    参数:\n",
    "    vocab_size: 词汇表大小\n",
    "    num_layers: 解码器层的数量\n",
    "    dim_model: 词向量的维度\n",
    "    num_heads: 注意力头的数量\n",
    "    dim_ff: 前馈神经网络的隐藏层维度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_layers, dim_model, num_heads, dim_ff, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        self.pos_encoding = positional_encoding(vocab_size, dim_model, device)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(dim_model, num_heads, dim_ff) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        x: 输入张量,形状为(batch_size, seq_len)\n",
    "        enc_output: 编码器输出张量,形状为(batch_size, seq_len, dim_model)\n",
    "        src_mask: 源序列掩码张量,形状为(batch_size, seq_len, seq_len)\n",
    "        tgt_mask: 目标序列掩码张量,形状为(batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "        output: 解码器输出张量,形状为(batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len, :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# 超参数\n",
    "vocab_size = len(chars)\n",
    "dim_model = 128\n",
    "num_layers = 5\n",
    "num_heads = 8\n",
    "dim_ff = 512\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 实例化编码器和解码器\n",
    "encoder = Encoder(vocab_size, num_layers, dim_model, num_heads, dim_ff, device)\n",
    "decoder = Decoder(vocab_size, num_layers, dim_model, num_heads, dim_ff, device)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "# 训练循环\n",
    "# 训练循环\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_src = src_data[i:i+batch_size].to(device)\n",
    "        batch_tgt = tgt_data[i:i+batch_size].to(device)\n",
    "\n",
    "        # 创建掩码\n",
    "        src_mask = torch.ones(batch_src.size(0), 1, batch_src.size(1)).to(device)\n",
    "        tgt_mask = torch.tril(torch.ones(batch_tgt.size(0), batch_tgt.size(1), batch_tgt.size(1))).to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        enc_output = encoder(batch_src, src_mask)\n",
    "        output = decoder(batch_tgt[:, :-1], enc_output, src_mask, tgt_mask[:, :-1, :-1])\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(output.reshape(-1, vocab_size), batch_tgt[:, 1:].reshape(-1))\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
